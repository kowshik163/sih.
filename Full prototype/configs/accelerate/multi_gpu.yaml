# Accelerate Configuration for Multi-GPU Training
# Use this config for distributed training on multiple GPUs

compute_environment: LOCAL_MACHINE
debug: false
distributed_type: MULTI_GPU
downcast_bf16: "no"
dynamo_backend: "NO"
fsdp_config: {}
gpu_ids: "all"
machine_rank: 0
main_training_function: main
mixed_precision: fp16
num_machines: 1
num_processes: 2  # Set to number of GPUs available
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
